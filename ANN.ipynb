{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Load data set \n",
    "file = \"name.xlsx\"\n",
    "spreadsheet = pd.ExcelFile(file)\n",
    "print(spreadsheet.sheet_names)\n",
    "data = spreadsheet.parse(\"name\")\n",
    "data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1496</th>\n",
       "      <th>358</th>\n",
       "      <th>854</th>\n",
       "      <th>87</th>\n",
       "      <th>623</th>\n",
       "      <th>131</th>\n",
       "      <th>707</th>\n",
       "      <th>1040</th>\n",
       "      <th>1462</th>\n",
       "      <th>1055</th>\n",
       "      <th>...</th>\n",
       "      <th>401</th>\n",
       "      <th>705</th>\n",
       "      <th>1550</th>\n",
       "      <th>79</th>\n",
       "      <th>1003</th>\n",
       "      <th>712</th>\n",
       "      <th>934</th>\n",
       "      <th>195</th>\n",
       "      <th>1565</th>\n",
       "      <th>845</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>7.000</td>\n",
       "      <td>6.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>2.000</td>\n",
       "      <td>6.00</td>\n",
       "      <td>...</td>\n",
       "      <td>4.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>2.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>5.000</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>85.000</td>\n",
       "      <td>246.00</td>\n",
       "      <td>150.00</td>\n",
       "      <td>121.000</td>\n",
       "      <td>135.00</td>\n",
       "      <td>163.00</td>\n",
       "      <td>235.00</td>\n",
       "      <td>102.00</td>\n",
       "      <td>155.000</td>\n",
       "      <td>85.00</td>\n",
       "      <td>...</td>\n",
       "      <td>85.00</td>\n",
       "      <td>114.00</td>\n",
       "      <td>113.000</td>\n",
       "      <td>140.000</td>\n",
       "      <td>62.00</td>\n",
       "      <td>165.00</td>\n",
       "      <td>61.00</td>\n",
       "      <td>85.00</td>\n",
       "      <td>91.000</td>\n",
       "      <td>147.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78.6</th>\n",
       "      <td>77.700</td>\n",
       "      <td>61.50</td>\n",
       "      <td>72.40</td>\n",
       "      <td>88.400</td>\n",
       "      <td>72.40</td>\n",
       "      <td>60.30</td>\n",
       "      <td>72.40</td>\n",
       "      <td>65.20</td>\n",
       "      <td>77.800</td>\n",
       "      <td>65.10</td>\n",
       "      <td>...</td>\n",
       "      <td>62.40</td>\n",
       "      <td>72.40</td>\n",
       "      <td>77.700</td>\n",
       "      <td>88.400</td>\n",
       "      <td>65.20</td>\n",
       "      <td>72.40</td>\n",
       "      <td>65.20</td>\n",
       "      <td>60.30</td>\n",
       "      <td>77.700</td>\n",
       "      <td>72.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>172.000</td>\n",
       "      <td>167.00</td>\n",
       "      <td>168.00</td>\n",
       "      <td>172.000</td>\n",
       "      <td>168.00</td>\n",
       "      <td>167.00</td>\n",
       "      <td>168.00</td>\n",
       "      <td>172.00</td>\n",
       "      <td>172.000</td>\n",
       "      <td>172.00</td>\n",
       "      <td>...</td>\n",
       "      <td>167.00</td>\n",
       "      <td>168.00</td>\n",
       "      <td>172.000</td>\n",
       "      <td>172.000</td>\n",
       "      <td>172.00</td>\n",
       "      <td>168.00</td>\n",
       "      <td>172.00</td>\n",
       "      <td>167.00</td>\n",
       "      <td>172.000</td>\n",
       "      <td>168.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>75.000</td>\n",
       "      <td>26.00</td>\n",
       "      <td>61.00</td>\n",
       "      <td>75.000</td>\n",
       "      <td>60.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>75.000</td>\n",
       "      <td>32.00</td>\n",
       "      <td>...</td>\n",
       "      <td>26.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>75.000</td>\n",
       "      <td>75.000</td>\n",
       "      <td>32.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>26.00</td>\n",
       "      <td>75.000</td>\n",
       "      <td>61.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.000</td>\n",
       "      <td>4.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>50.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>45.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>30.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>45.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>80.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.4</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.6</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.6</th>\n",
       "      <td>6.600</td>\n",
       "      <td>7.40</td>\n",
       "      <td>7.10</td>\n",
       "      <td>6.200</td>\n",
       "      <td>7.10</td>\n",
       "      <td>7.40</td>\n",
       "      <td>7.10</td>\n",
       "      <td>7.10</td>\n",
       "      <td>6.600</td>\n",
       "      <td>7.00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.40</td>\n",
       "      <td>7.10</td>\n",
       "      <td>6.600</td>\n",
       "      <td>6.200</td>\n",
       "      <td>7.10</td>\n",
       "      <td>7.10</td>\n",
       "      <td>7.10</td>\n",
       "      <td>7.40</td>\n",
       "      <td>6.600</td>\n",
       "      <td>7.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.7</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>200.000</td>\n",
       "      <td>150.00</td>\n",
       "      <td>140.00</td>\n",
       "      <td>200.000</td>\n",
       "      <td>120.00</td>\n",
       "      <td>150.00</td>\n",
       "      <td>120.00</td>\n",
       "      <td>140.00</td>\n",
       "      <td>200.000</td>\n",
       "      <td>120.00</td>\n",
       "      <td>...</td>\n",
       "      <td>150.00</td>\n",
       "      <td>140.00</td>\n",
       "      <td>200.000</td>\n",
       "      <td>200.000</td>\n",
       "      <td>120.00</td>\n",
       "      <td>140.00</td>\n",
       "      <td>120.00</td>\n",
       "      <td>150.00</td>\n",
       "      <td>200.000</td>\n",
       "      <td>140.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>60.000</td>\n",
       "      <td>90.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>60.000</td>\n",
       "      <td>60.00</td>\n",
       "      <td>90.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>60.000</td>\n",
       "      <td>60.00</td>\n",
       "      <td>...</td>\n",
       "      <td>90.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>60.000</td>\n",
       "      <td>60.000</td>\n",
       "      <td>60.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>90.00</td>\n",
       "      <td>60.000</td>\n",
       "      <td>60.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46.153</th>\n",
       "      <td>46.153</td>\n",
       "      <td>81.81</td>\n",
       "      <td>47.36</td>\n",
       "      <td>46.153</td>\n",
       "      <td>47.36</td>\n",
       "      <td>81.81</td>\n",
       "      <td>47.36</td>\n",
       "      <td>42.85</td>\n",
       "      <td>46.153</td>\n",
       "      <td>42.85</td>\n",
       "      <td>...</td>\n",
       "      <td>81.81</td>\n",
       "      <td>47.36</td>\n",
       "      <td>46.153</td>\n",
       "      <td>46.153</td>\n",
       "      <td>42.85</td>\n",
       "      <td>47.36</td>\n",
       "      <td>42.85</td>\n",
       "      <td>81.81</td>\n",
       "      <td>46.153</td>\n",
       "      <td>47.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17 rows × 1577 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           1496    358     854      87      623     131     707     1040  \\\n",
       "5         5.000    4.00    6.00    7.000    6.00    4.00    6.00    8.00   \n",
       "114      85.000  246.00  150.00  121.000  135.00  163.00  235.00  102.00   \n",
       "0         0.000    1.00    0.00    0.000    0.00    1.00    0.00    1.00   \n",
       "78.6     77.700   61.50   72.40   88.400   72.40   60.30   72.40   65.20   \n",
       "172     172.000  167.00  168.00  172.000  168.00  167.00  168.00  172.00   \n",
       "75       75.000   26.00   61.00   75.000   60.00   26.00   60.00   32.00   \n",
       "0.1       0.000    2.00    0.00    0.000    0.00    2.00    0.00    0.00   \n",
       "0.2       0.000    4.00    4.00    2.000    0.00    2.00    0.00    2.00   \n",
       "0.3       0.000    0.00    0.00    0.000   50.00    0.00    0.00   45.00   \n",
       "0.4       0.000    0.00    0.00    0.000    2.00    0.00    0.00    1.00   \n",
       "0.5       0.000    0.00    0.00    0.000    0.00    0.00    0.00    0.00   \n",
       "0.6       0.000    0.00    0.00    0.000    0.00    0.00    0.00    0.00   \n",
       "6.6       6.600    7.40    7.10    6.200    7.10    7.40    7.10    7.10   \n",
       "0.7       0.000    0.00    0.00    0.000    0.00    0.00    0.00    0.00   \n",
       "200     200.000  150.00  140.00  200.000  120.00  150.00  120.00  140.00   \n",
       "60       60.000   90.00   60.00   60.000   60.00   90.00   60.00   60.00   \n",
       "46.153   46.153   81.81   47.36   46.153   47.36   81.81   47.36   42.85   \n",
       "\n",
       "           1462    1055   ...      401     705      1550     79      1003  \\\n",
       "5         2.000    6.00   ...      4.00    6.00    2.000    2.000    6.00   \n",
       "114     155.000   85.00   ...     85.00  114.00  113.000  140.000   62.00   \n",
       "0         0.000    1.00   ...      1.00    0.00    0.000    0.000    1.00   \n",
       "78.6     77.800   65.10   ...     62.40   72.40   77.700   88.400   65.20   \n",
       "172     172.000  172.00   ...    167.00  168.00  172.000  172.000  172.00   \n",
       "75       75.000   32.00   ...     26.00   60.00   75.000   75.000   32.00   \n",
       "0.1       0.000    0.00   ...      0.00    0.00    0.000    0.000    0.00   \n",
       "0.2       4.000    0.00   ...      0.00    2.00    4.000    4.000    0.00   \n",
       "0.3       0.000   30.00   ...      0.00    0.00    0.000    0.000   45.00   \n",
       "0.4       0.000    3.00   ...      0.00    0.00    0.000    0.000    1.00   \n",
       "0.5       0.000    0.00   ...      1.00    0.00    0.000    0.000    0.00   \n",
       "0.6       0.000    0.00   ...      0.00    0.00    0.000    0.000    0.00   \n",
       "6.6       6.600    7.00   ...      7.40    7.10    6.600    6.200    7.10   \n",
       "0.7       0.000    0.00   ...      0.00    0.00    0.000    0.000    0.00   \n",
       "200     200.000  120.00   ...    150.00  140.00  200.000  200.000  120.00   \n",
       "60       60.000   60.00   ...     90.00   60.00   60.000   60.000   60.00   \n",
       "46.153   46.153   42.85   ...     81.81   47.36   46.153   46.153   42.85   \n",
       "\n",
       "          712     934     195      1565    845   \n",
       "5         6.00    6.00    4.00    5.000    6.00  \n",
       "114     165.00   61.00   85.00   91.000  147.00  \n",
       "0         0.00    1.00    1.00    0.000    0.00  \n",
       "78.6     72.40   65.20   60.30   77.700   72.40  \n",
       "172     168.00  172.00  167.00  172.000  168.00  \n",
       "75       60.00   32.00   26.00   75.000   61.00  \n",
       "0.1       0.00    1.00    2.00    0.000    0.00  \n",
       "0.2       4.00    0.00    4.00    0.000    4.00  \n",
       "0.3       0.00   80.00    0.00    0.000    0.00  \n",
       "0.4       0.00    2.00    0.00    0.000    0.00  \n",
       "0.5       0.00    0.00    0.00    0.000    0.00  \n",
       "0.6       0.00    0.00    0.00    0.000    0.00  \n",
       "6.6       7.10    7.10    7.40    6.600    7.10  \n",
       "0.7       0.00    0.00    0.00    0.000    0.00  \n",
       "200     140.00  120.00  150.00  200.000  140.00  \n",
       "60       60.00   60.00   90.00   60.000   60.00  \n",
       "46.153   47.36   42.85   81.81   46.153   47.36  \n",
       "\n",
       "[17 rows x 1577 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shuffle data\n",
    "data = data.sample(frac=1)\n",
    "data = data.T\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data.iloc[1:, 0:1100]\n",
    "Y_train = data.iloc[:1, 0:1100]\n",
    "X_test = data.iloc[1:, 1101:1576]\n",
    "Y_test = data.iloc[:1, 1101:1576]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Create placeholders for X and Y to use later\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, [n_x, None], name = \"X\")\n",
    "    Y = tf.placeholder(tf.float32, [n_y, None], name = \"Y\")\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    \"\"\"\n",
    "    Initialize parameters to build a deep neural network with 4 layers, each one with 30 units:\n",
    "    W1 = [30, 16]\n",
    "    b1 = [30, 1]\n",
    "    W2 = [30, 30]\n",
    "    b2 = [30, 1]\n",
    "    W3 = [30, 30]\n",
    "    b3 = [30, 1]\n",
    "    W4 = [30, 30]\n",
    "    b4 = [30, 1]\n",
    "    W5 = [1, 30]\n",
    "    b5 = [1, 1]\n",
    "    \"\"\"\n",
    "    W1 = tf.get_variable(\"W1\", [30, 16], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.get_variable(\"b1\", [30, 1], initializer = tf.zeros_initializer())\n",
    "    W2 = tf.get_variable(\"W2\", [30, 30], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.get_variable(\"b2\", [30, 1], initializer = tf.zeros_initializer())\n",
    "    W3 = tf.get_variable(\"W3\", [30, 30], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.get_variable(\"b3\", [30, 1], initializer = tf.zeros_initializer())\n",
    "    W4 = tf.get_variable(\"W4\", [30, 30], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b4 = tf.get_variable(\"b4\", [30, 1], initializer = tf.zeros_initializer())\n",
    "    W5 = tf.get_variable(\"W6\", [1, 30], initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b5 = tf.get_variable(\"b6\", [1, 1], initializer = tf.zeros_initializer())\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3,\n",
    "                  \"W4\": W4,\n",
    "                  \"b4\": b4,\n",
    "                  \"W5\": W5,\n",
    "                  \"b5\": b5}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for themodel: LINEAR -> RELU x5 -> LINEAR\n",
    "    \n",
    "    \"\"\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    W4 = parameters[\"W4\"]\n",
    "    b4 = parameters[\"b4\"]\n",
    "    W5 = parameters[\"W5\"]\n",
    "    b5 = parameters[\"b5\"]\n",
    "    \n",
    "    epsilon = 1e-3\n",
    "    \n",
    "    Z1 = tf.add(tf.matmul(W1, X), b1)\n",
    "    Z1_norm = tf.layers.batch_normalization(Z1, axis = 0, momentum = 0.99, epsilon = epsilon, center = True, scale = True)\n",
    "    A1 = tf.nn.relu(Z1_norm)\n",
    "    Z2 = tf.add(tf.matmul(W2, A1), b2)\n",
    "    Z2_norm = tf.layers.batch_normalization(Z2, axis = 0, momentum = 0.99, epsilon = epsilon, center = True, scale = True)\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(W3, A2), b3)\n",
    "    Z3_norm = tf.layers.batch_normalization(Z3, axis = 0, momentum = 0.99, epsilon = epsilon, center = True, scale = True)\n",
    "    A3 = tf.nn.relu(Z3_norm)\n",
    "    Z4 = tf.add(tf.matmul(W4, A3), b4)\n",
    "    Z4_norm = tf.layers.batch_normalization(Z4, axis = 0, momentum = 0.99, epsilon = epsilon, center = True, scale = True)\n",
    "    A4 = tf.nn.relu(Z4_norm)\n",
    "    Z5 = tf.add(tf.matmul(W5, A4), b5)\n",
    "\n",
    "    return Z5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z5, Y):\n",
    "    \"\"\"\n",
    "    Compute the cost\n",
    "    \n",
    "    \"\"\"\n",
    "    labels = Y\n",
    "    predictions = Z5\n",
    "    \n",
    "    cost = tf.losses.huber_loss(labels = labels, predictions = predictions)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001, num_iters = 15000, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implement a four layer neural network: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> RELU -> LINEAR\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()\n",
    "    \n",
    "    (n_x, m) = X_train.shape             # n_x: input size, m: number of examples in train set\n",
    "    n_y = Y_train.shape[0]               # n_y: output size\n",
    "        \n",
    "    costs = []                           # To keep track of the cost\n",
    "    \n",
    "    \n",
    "    # Create placeholders of shape (n_x, n_y)\n",
    "    \n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters()\n",
    "    \n",
    "    # Build the forward propagation\n",
    "    Z5 = forward_propagation(X, parameters)\n",
    "    \n",
    "    # Compute the cost\n",
    "    cost = compute_cost(Z5, Y)\n",
    "    \n",
    "    # Backpropagation with AdamOptimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    \n",
    "    # Initialize all the global variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for iters in range(num_iters):\n",
    "            \n",
    "            iter_cost = 0\n",
    "            _, iter_cost = sess.run([optimizer, cost], feed_dict = {X: X_train, Y: Y_train})\n",
    "            \n",
    "            iter_cost += iter_cost/num_iters\n",
    "    \n",
    "        # Print the cost every epoch\n",
    "            if print_cost == True and iters % 100 == 0:\n",
    "                print(\"Cost after iteration %i: %f\" % (iters, iter_cost))\n",
    "            if print_cost == True and iters % 5 == 0:\n",
    "                costs.append(iter_cost)\n",
    "    # Plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel(\"cost\")\n",
    "        plt.xlabel(\"Iterations(per tens)\")\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "    \n",
    "    # Save parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print(\"Parameters have been trained\")\n",
    "    \n",
    "    #Calculate de correct prediction\n",
    "    \n",
    "        correct_prediction = np.abs(Y - Z5)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        max_error = np.argmax(correct_prediction)\n",
    "        \n",
    "        print(\"Train error: \", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print(\"Test error: \", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        print(\"Max error: \", np.amax(correct_prediction.eval({X: X_test, Y: Y_test})))\n",
    "        print(\"Index: \", np.argmax(correct_prediction.eval({X: X_test, Y: Y_test})))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 34.989069\n",
      "Cost after iteration 100: 10.289138\n",
      "Cost after iteration 200: 0.992096\n",
      "Cost after iteration 300: 0.560466\n",
      "Cost after iteration 400: 0.450079\n",
      "Cost after iteration 500: 0.412676\n",
      "Cost after iteration 600: 0.394824\n",
      "Cost after iteration 700: 0.383341\n",
      "Cost after iteration 800: 0.375386\n",
      "Cost after iteration 900: 0.370570\n",
      "Cost after iteration 1000: 0.366524\n",
      "Cost after iteration 1100: 0.363097\n",
      "Cost after iteration 1200: 0.360269\n",
      "Cost after iteration 1300: 0.357585\n",
      "Cost after iteration 1400: 0.354891\n",
      "Cost after iteration 1500: 0.352452\n",
      "Cost after iteration 1600: 0.349839\n",
      "Cost after iteration 1700: 0.347456\n",
      "Cost after iteration 1800: 0.345038\n",
      "Cost after iteration 1900: 0.342512\n",
      "Cost after iteration 2000: 0.339751\n",
      "Cost after iteration 2100: 0.336951\n",
      "Cost after iteration 2200: 0.333999\n",
      "Cost after iteration 2300: 0.330674\n",
      "Cost after iteration 2400: 0.326563\n",
      "Cost after iteration 2500: 0.322331\n",
      "Cost after iteration 2600: 0.317579\n",
      "Cost after iteration 2700: 0.312000\n",
      "Cost after iteration 2800: 0.305566\n",
      "Cost after iteration 2900: 0.297208\n",
      "Cost after iteration 3000: 0.287088\n",
      "Cost after iteration 3100: 0.275645\n",
      "Cost after iteration 3200: 0.260628\n",
      "Cost after iteration 3300: 0.246738\n",
      "Cost after iteration 3400: 0.234967\n",
      "Cost after iteration 3500: 0.228771\n",
      "Cost after iteration 3600: 0.225484\n",
      "Cost after iteration 3700: 0.222845\n",
      "Cost after iteration 3800: 0.219713\n",
      "Cost after iteration 3900: 0.216983\n",
      "Cost after iteration 4000: 0.214753\n",
      "Cost after iteration 4100: 0.211013\n",
      "Cost after iteration 4200: 0.207341\n",
      "Cost after iteration 4300: 0.204301\n",
      "Cost after iteration 4400: 0.198225\n",
      "Cost after iteration 4500: 0.173707\n",
      "Cost after iteration 4600: 0.153949\n",
      "Cost after iteration 4700: 0.139689\n",
      "Cost after iteration 4800: 0.130317\n",
      "Cost after iteration 4900: 0.122004\n",
      "Cost after iteration 5000: 0.110361\n",
      "Cost after iteration 5100: 0.098571\n",
      "Cost after iteration 5200: 0.088522\n",
      "Cost after iteration 5300: 0.079654\n",
      "Cost after iteration 5400: 0.072445\n",
      "Cost after iteration 5500: 0.065701\n",
      "Cost after iteration 5600: 0.059140\n",
      "Cost after iteration 5700: 0.052995\n",
      "Cost after iteration 5800: 0.048155\n",
      "Cost after iteration 5900: 0.043627\n",
      "Cost after iteration 6000: 0.040458\n",
      "Cost after iteration 6100: 0.037630\n",
      "Cost after iteration 6200: 0.035640\n",
      "Cost after iteration 6300: 0.034057\n",
      "Cost after iteration 6400: 0.032488\n",
      "Cost after iteration 6500: 0.031510\n",
      "Cost after iteration 6600: 0.030429\n",
      "Cost after iteration 6700: 0.029639\n",
      "Cost after iteration 6800: 0.028926\n",
      "Cost after iteration 6900: 0.028362\n",
      "Cost after iteration 7000: 0.027721\n",
      "Cost after iteration 7100: 0.027261\n",
      "Cost after iteration 7200: 0.026900\n",
      "Cost after iteration 7300: 0.026306\n",
      "Cost after iteration 7400: 0.025959\n",
      "Cost after iteration 7500: 0.025509\n",
      "Cost after iteration 7600: 0.025317\n",
      "Cost after iteration 7700: 0.024667\n",
      "Cost after iteration 7800: 0.024356\n",
      "Cost after iteration 7900: 0.023972\n",
      "Cost after iteration 8000: 0.023649\n",
      "Cost after iteration 8100: 0.023375\n",
      "Cost after iteration 8200: 0.023095\n",
      "Cost after iteration 8300: 0.022856\n",
      "Cost after iteration 8400: 0.022559\n",
      "Cost after iteration 8500: 0.022410\n",
      "Cost after iteration 8600: 0.021918\n",
      "Cost after iteration 8700: 0.021626\n",
      "Cost after iteration 8800: 0.021565\n",
      "Cost after iteration 8900: 0.021158\n",
      "Cost after iteration 9000: 0.020854\n",
      "Cost after iteration 9100: 0.020724\n",
      "Cost after iteration 9200: 0.020445\n",
      "Cost after iteration 9300: 0.020131\n",
      "Cost after iteration 9400: 0.019902\n",
      "Cost after iteration 9500: 0.019782\n",
      "Cost after iteration 9600: 0.019608\n",
      "Cost after iteration 9700: 0.019257\n",
      "Cost after iteration 9800: 0.018988\n",
      "Cost after iteration 9900: 0.018914\n",
      "Cost after iteration 10000: 0.018525\n",
      "Cost after iteration 10100: 0.018478\n",
      "Cost after iteration 10200: 0.018345\n",
      "Cost after iteration 10300: 0.017999\n",
      "Cost after iteration 10400: 0.017839\n",
      "Cost after iteration 10500: 0.017413\n",
      "Cost after iteration 10600: 0.017173\n",
      "Cost after iteration 10700: 0.017241\n",
      "Cost after iteration 10800: 0.016724\n",
      "Cost after iteration 10900: 0.016571\n",
      "Cost after iteration 11000: 0.016337\n",
      "Cost after iteration 11100: 0.016159\n",
      "Cost after iteration 11200: 0.015908\n",
      "Cost after iteration 11300: 0.015767\n",
      "Cost after iteration 11400: 0.015723\n",
      "Cost after iteration 11500: 0.015353\n",
      "Cost after iteration 11600: 0.015243\n",
      "Cost after iteration 11700: 0.015352\n",
      "Cost after iteration 11800: 0.014918\n",
      "Cost after iteration 11900: 0.014753\n",
      "Cost after iteration 12000: 0.014718\n",
      "Cost after iteration 12100: 0.014495\n",
      "Cost after iteration 12200: 0.014570\n",
      "Cost after iteration 12300: 0.014312\n",
      "Cost after iteration 12400: 0.014278\n",
      "Cost after iteration 12500: 0.013976\n",
      "Cost after iteration 12600: 0.013884\n",
      "Cost after iteration 12700: 0.013666\n",
      "Cost after iteration 12800: 0.013795\n",
      "Cost after iteration 12900: 0.013384\n",
      "Cost after iteration 13000: 0.013303\n",
      "Cost after iteration 13100: 0.013203\n",
      "Cost after iteration 13200: 0.012982\n",
      "Cost after iteration 13300: 0.012659\n",
      "Cost after iteration 13400: 0.012556\n",
      "Cost after iteration 13500: 0.012392\n",
      "Cost after iteration 13600: 0.012413\n",
      "Cost after iteration 13700: 0.012183\n",
      "Cost after iteration 13800: 0.011989\n",
      "Cost after iteration 13900: 0.011818\n",
      "Cost after iteration 14000: 0.011675\n",
      "Cost after iteration 14100: 0.011761\n",
      "Cost after iteration 14200: 0.011469\n",
      "Cost after iteration 14300: 0.011298\n",
      "Cost after iteration 14400: 0.011267\n",
      "Cost after iteration 14500: 0.011159\n",
      "Cost after iteration 14600: 0.011106\n",
      "Cost after iteration 14700: 0.011291\n",
      "Cost after iteration 14800: 0.011027\n",
      "Cost after iteration 14900: 0.011207\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters have been trained\n",
      "Train error:  0.08161892\n",
      "Test error:  0.16770983\n",
      "Max error:  2.7564127\n",
      "Index:  7\n"
     ]
    }
   ],
   "source": [
    "parameters = model(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
